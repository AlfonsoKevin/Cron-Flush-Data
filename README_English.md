<p align="center">
    <a href="" target="_blank">
      <img src="./imgs/cover.png" width="280" />
    </a>
</p>
<h1 align="center">Cron-Flush-Data</h1>
<p align="center"><strong>Scheduled task: Users refresh data in the database and cache<br><em>Continuously updatingï½</em></strong></p>
<div align="center">
    <a href=""><img src="https://img.shields.io/badge/github-é¡¹ç›®åœ°å€-yellow.svg?style=plasticr"></a></div>



[ä¸­æ–‡|English]

## IntroductionğŸ˜

â€‹	Assuming that you have difficulty deploying a project online, the essence of this project is to showcase and be satisfied with the data in the online database ğŸ˜ã€‚ Some features in the project require you to consider regularly refreshing the data. For example, a certain feature has a limit on the number of times it can be accessed, but you haven't connected to any account services, so the number of times is pre configured by you. But if the usage limit is reached, it will have a very poor experience for others later on, and it will also affect the display itself.

â€‹	The project provides a scheduled task for updating database data, and of course, if cache is configured, it also provides a configuration for refreshing the cache.

> Precondition:
>1. Linux environment
> 2. Used to showcase projects
>3. MySQL and Redis deployed in Docker
> 4. Ensure that Docker containers are in a running state during Cron execution.

## Project DirectoryğŸ“‡

```txt
â”œâ”€ğŸ“„ cron_tab.md                   # å®šæ—¶ä»»åŠ¡è¯´æ˜
â”œâ”€ğŸ“„ log_change.md                 # æ—¥å¿—è½®æ¢
â”œâ”€ğŸ“„ README.md
â”œâ”€ğŸ“„ README_English.md
â””â”€ğŸ“ shell                         # Shellè„šæœ¬
  â”œâ”€ğŸ“„ import_sql.sh               # SQL
  â””â”€ğŸ“„ import_sql_redis.sh         # SQLå’ŒRedisçš„
```





## Eating GuideğŸ§­

Use MySQL SQL files to import data for data synchronization, and combine Redis to clear cache. Each database corresponds to a file, and if there are multiple databases corresponding to multiple files.

The entire process is as follows:

1.Create a scheduled task folder and grant permissions; Add SQL files to the folder, one file corresponds to one database

2.Modify configuration (MySQL container name, SQL file path, password, account, Redis related configuration...)

3.Test directly using scripts`/ import_sql.sh`

4.View the running results /logs

5.[Regularly execute using cron expressions ğŸ¤–](./cron_tab.md)

6.[Log rotation â­](./log_change.md)



Provided two versions

### Version 1:

Timed task to refresh database data, multiple databases (can adjust the number of elements in the array), without caching.

`import_sql.sh`

```sh
#!/bin/bash

# Docker å®¹å™¨å’Œæ•°æ®åº“é…ç½®
DOCKER_CONTAINER="your_mysql_container_name"  # Docker å®¹å™¨åç§°
DB_USER="your_db_user"                        # æ•°æ®åº“ç”¨æˆ·å
DB_PASSWORD="your_db_password"                # æ•°æ®åº“å¯†ç 

# å®šä¹‰ä¸‰ä¸ªæ•°æ®åº“åŠå…¶å¯¹åº”çš„ SQL æ–‡ä»¶è·¯å¾„
declare -A DATABASES=(
  ["database1"]="/path/to/database1.sql"     # æ•°æ®åº“1åç§°å’Œå¯¹åº”çš„SQLæ–‡ä»¶
  ["database2"]="/path/to/database2.sql"     # æ•°æ®åº“2åç§°å’Œå¯¹åº”çš„SQLæ–‡ä»¶
  ["database3"]="/path/to/database3.sql"     # æ•°æ®åº“3åç§°å’Œå¯¹åº”çš„SQLæ–‡ä»¶
)

# éå†æ‰€æœ‰æ•°æ®åº“å¹¶æ‰§è¡Œå¯¹åº”çš„ SQL æ–‡ä»¶
for db_name in "${!DATABASES[@]}"; do
  sql_file="${DATABASES[$db_name]}"
  echo "æ­£åœ¨åŒæ­¥æ•°æ®åº“ [$db_name]ï¼Œä½¿ç”¨æ–‡ä»¶: $sql_file"

  # æ£€æŸ¥ SQL æ–‡ä»¶æ˜¯å¦å­˜åœ¨
  if [ ! -f "$sql_file" ]; then
    echo "é”™è¯¯: SQL æ–‡ä»¶ $sql_file ä¸å­˜åœ¨ï¼"
    continue  # è·³è¿‡å½“å‰å¾ªç¯ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªæ•°æ®åº“
  fi

  # æ‰§è¡Œå¯¼å…¥å‘½ä»¤
  docker exec -i $DOCKER_CONTAINER mysql -u$DB_USER -p$DB_PASSWORD $db_name < $sql_file

  # æ£€æŸ¥æ‰§è¡Œç»“æœ
  if [ $? -eq 0 ]; then
    echo "[$db_name] åŒæ­¥æˆåŠŸ"
  else
    echo "[$db_name] åŒæ­¥å¤±è´¥ï¼"
  fi
done
```



### Version 2:

**Introduced Redis and used a [delayed double deletion](#Precautions ğŸ‘Œ) strategy, first deleting the cache, updating the database, and then delaying the deletion of the cache.** Added refresh cache on the basis of multi database synchronization.

`import_sql_redis.sh`

```sh
#!/bin/bash

# --------------------------
# Docker å®¹å™¨é…ç½®
# --------------------------
DOCKER_MYSQL="your_mysql_container"    # MySQL å®¹å™¨åç§°
DOCKER_REDIS="your_redis_container"    # Redis å®¹å™¨åç§°

# --------------------------
# MySQL æ•°æ®åº“é…ç½®
# --------------------------
DB_USER="your_db_user"
DB_PASSWORD="your_db_password"
declare -A DATABASES=(
  ["database1"]="/path/to/database1.sql"
  ["database2"]="/path/to/database2.sql"
  ["database3"]="/path/to/database3.sql"
)

# --------------------------
# å»¶æ—¶åŒåˆ å‚æ•°
# --------------------------
DELAY_SECONDS=5  # ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œæ ¹æ®ä¸šåŠ¡éœ€æ±‚è°ƒæ•´

# --------------------------
# ä¸»é€»è¾‘
# --------------------------
error_flag=0  # å…¨å±€é”™è¯¯æ ‡è®°

# æ­¥éª¤ 1: ç¬¬ä¸€æ¬¡æ¸…ç† Redis ç¼“å­˜
echo "[1/4] ç¬¬ä¸€æ¬¡æ¸…ç† Redis ç¼“å­˜..."
docker exec $DOCKER_REDIS redis-cli flushall >/dev/null 2>&1
if [ $? -eq 0 ]; then
  echo "ç¬¬ä¸€æ¬¡æ¸…ç†æˆåŠŸ"
else
  echo "ç¬¬ä¸€æ¬¡æ¸…ç†å¤±è´¥ï¼"
  error_flag=1  # å¯é€‰é¡¹ï¼šå¦‚æœç¬¬ä¸€æ¬¡æ¸…ç†å¤±è´¥ç›´æ¥ç»ˆæ­¢ä»»åŠ¡ï¼Œç§»é™¤æ­¤è¡Œåˆ™ä¸ç»ˆæ­¢
fi

# æ­¥éª¤ 2: åŒæ­¥ MySQL æ•°æ®åº“
echo "[2/4] å¼€å§‹åŒæ­¥æ•°æ®åº“..."
for db_name in "${!DATABASES[@]}"; do
  sql_file="${DATABASES[$db_name]}"
  if [ ! -f "$sql_file" ]; then
    echo "é”™è¯¯: SQL æ–‡ä»¶ $sql_file ä¸å­˜åœ¨ï¼"
    error_flag=1
    continue
  fi

  docker exec -i $DOCKER_MYSQL mysql -u$DB_USER -p$DB_PASSWORD $db_name < "$sql_file"
  if [ $? -ne 0 ]; then
    echo "[$db_name] åŒæ­¥å¤±è´¥ï¼"
    error_flag=1
  else
    echo "[$db_name] åŒæ­¥æˆåŠŸ"
  fi
done

# æ­¥éª¤ 3-4: ä»…å½“æ•°æ®åº“åŒæ­¥æˆåŠŸæ—¶æ‰§è¡Œç¬¬äºŒæ¬¡æ¸…ç†
if [ $error_flag -eq 0 ]; then
  echo "[3/4] ç­‰å¾… ${DELAY_SECONDS} ç§’..."
  sleep $DELAY_SECONDS

  echo "[4/4] æ‰§è¡Œç¬¬äºŒæ¬¡ Redis ç¼“å­˜æ¸…ç†..."
  docker exec $DOCKER_REDIS redis-cli flushall >/dev/null 2>&1
  if [ $? -eq 0 ]; then
    echo "ç¬¬äºŒæ¬¡æ¸…ç†æˆåŠŸ"
  else
    echo "ç¬¬äºŒæ¬¡æ¸…ç†å¤±è´¥ï¼"
  fi
else
  echo "[3/4] æ•°æ®åº“åŒæ­¥å¤±è´¥ï¼Œè·³è¿‡ç¬¬äºŒæ¬¡ç¼“å­˜æ¸…ç†"
fi
```





## Precautions ğŸ‘Œ

The strategy used is` delayed double deletion `, which cannot guarantee strong consistency of double writes due to the uncertainty of the delay time. Please refer to the database and cache double write consistency issues on your own to demonstrate the project. It can be manually adjusted based on experience.

------

Thank you for your attention and support to this project! ğŸ•µï¸â€â™€ï¸





